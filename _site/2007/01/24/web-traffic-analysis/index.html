<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Web Traffic Analysis | Your awesome title</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Web Traffic Analysis" />
<meta name="author" content="Ben" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In the realm of analyzing traffic that your web-site is generating, there are two main approaches. Log-file analysis &amp; real-time analysis (I don&#39;t know if that&#39;s an industry term, but that&#39;s what I&#39;m going to call it). Real-Time Analysis As a general rule, the real-time analysis tools use javascript to track visitors to your website. Pasting a bit of javascript into the header of your HTML pages (or templates) means that every time a page is loaded, your website will send a call to a remote database somewhere with information about the visitor. Logging into the account you&#39;ve created at the remote site will allow you to view statistics with varying levels of details depending on the service. Due to the use of Javascript to track and record statistics, these are completely platform independent. Omniture&#39;s Site Catalyst - Enterprise level analytics. As far as I know, Site Catalyst is really the only player at this level in the market. I don&#39;t know a whole lot about it, but as I get access to it later on, I&#39;m sure I&#39;ll be writing more about it. Google Analytics - As far as I know, this is the weapon of choice for those that don&#39;t have the resources to implement and use site-catalyst but still want real-time analysis of their web traffic Microsoft&#39;s Gatineau - As of this writing, Gatineau isn&#39;t live yet, so we don&#39;t exactly know how it will work, but you can bet their trying to enter the Google Analytics market. Details are sparse, but there are a few details over at searchenginejournal.com Others - There are many other tools available. Some of the ones I know of are Sitetracker, OneStat (pay service), and StatCounter Log-file Analysis The alternate method of tracking visitors to your site is to use log-file analysis. Every time your HTTP server sends out a page to a web browser, the request is stored in a log file. A log file is simply a text file on your server that contains IP addresses, times, pages requested, and any other number of statistics. Parsing through the log files and comparing certain bits of data to others allows a detailed picture of the traffic on your website. Because these tools actually live on the servers andlook through log files, they are OS dependent. AWStats - As far as I know, AWStats is the major player in this market. It&#39;s an open-source tool that has been around since before I was doing web development. I don&#39;t have extensive experience with AWStats, but my understanding is that the level of detail is not quite on par with Google Analytics. AWStats works by running a PERL (maybe Python) script that parses through your server log files to generate static HTML pages. A cron job runs the script whenever you schedule it, and then the stats are available at http://yourdomain/stats. There is also a CGI interface that will do it in real-time for you. Others - There are many tools that do this type of job. See the Wikipedia for more options in this arena. A post from Jimmy Zimmerman highlights one issue with log-file analysis. Blogs and other types of web-sites that encourage user feedback have forms for posting comments, etc. These comment forms are highly targeted by comment spammers that use this method of feedback for free advertising. Since these spams consist of valid Request/Response pairs, they pollute your log files with false data. If you use log-file analysis to track statistics on your site, be aware of this and take steps to cleanse your logs to produce accurate stats." />
<meta property="og:description" content="In the realm of analyzing traffic that your web-site is generating, there are two main approaches. Log-file analysis &amp; real-time analysis (I don&#39;t know if that&#39;s an industry term, but that&#39;s what I&#39;m going to call it). Real-Time Analysis As a general rule, the real-time analysis tools use javascript to track visitors to your website. Pasting a bit of javascript into the header of your HTML pages (or templates) means that every time a page is loaded, your website will send a call to a remote database somewhere with information about the visitor. Logging into the account you&#39;ve created at the remote site will allow you to view statistics with varying levels of details depending on the service. Due to the use of Javascript to track and record statistics, these are completely platform independent. Omniture&#39;s Site Catalyst - Enterprise level analytics. As far as I know, Site Catalyst is really the only player at this level in the market. I don&#39;t know a whole lot about it, but as I get access to it later on, I&#39;m sure I&#39;ll be writing more about it. Google Analytics - As far as I know, this is the weapon of choice for those that don&#39;t have the resources to implement and use site-catalyst but still want real-time analysis of their web traffic Microsoft&#39;s Gatineau - As of this writing, Gatineau isn&#39;t live yet, so we don&#39;t exactly know how it will work, but you can bet their trying to enter the Google Analytics market. Details are sparse, but there are a few details over at searchenginejournal.com Others - There are many other tools available. Some of the ones I know of are Sitetracker, OneStat (pay service), and StatCounter Log-file Analysis The alternate method of tracking visitors to your site is to use log-file analysis. Every time your HTTP server sends out a page to a web browser, the request is stored in a log file. A log file is simply a text file on your server that contains IP addresses, times, pages requested, and any other number of statistics. Parsing through the log files and comparing certain bits of data to others allows a detailed picture of the traffic on your website. Because these tools actually live on the servers andlook through log files, they are OS dependent. AWStats - As far as I know, AWStats is the major player in this market. It&#39;s an open-source tool that has been around since before I was doing web development. I don&#39;t have extensive experience with AWStats, but my understanding is that the level of detail is not quite on par with Google Analytics. AWStats works by running a PERL (maybe Python) script that parses through your server log files to generate static HTML pages. A cron job runs the script whenever you schedule it, and then the stats are available at http://yourdomain/stats. There is also a CGI interface that will do it in real-time for you. Others - There are many tools that do this type of job. See the Wikipedia for more options in this arena. A post from Jimmy Zimmerman highlights one issue with log-file analysis. Blogs and other types of web-sites that encourage user feedback have forms for posting comments, etc. These comment forms are highly targeted by comment spammers that use this method of feedback for free advertising. Since these spams consist of valid Request/Response pairs, they pollute your log files with false data. If you use log-file analysis to track statistics on your site, be aware of this and take steps to cleanse your logs to produce accurate stats." />
<link rel="canonical" href="http://localhost:4000/2007/01/24/web-traffic-analysis/" />
<meta property="og:url" content="http://localhost:4000/2007/01/24/web-traffic-analysis/" />
<meta property="og:site_name" content="Your awesome title" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2007-01-24T00:04:56-07:00" />
<script type="application/ld+json">
{"headline":"Web Traffic Analysis","dateModified":"2007-01-24T00:04:56-07:00","datePublished":"2007-01-24T00:04:56-07:00","url":"http://localhost:4000/2007/01/24/web-traffic-analysis/","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2007/01/24/web-traffic-analysis/"},"author":{"@type":"Person","name":"Ben"},"description":"In the realm of analyzing traffic that your web-site is generating, there are two main approaches. Log-file analysis &amp; real-time analysis (I don&#39;t know if that&#39;s an industry term, but that&#39;s what I&#39;m going to call it). Real-Time Analysis As a general rule, the real-time analysis tools use javascript to track visitors to your website. Pasting a bit of javascript into the header of your HTML pages (or templates) means that every time a page is loaded, your website will send a call to a remote database somewhere with information about the visitor. Logging into the account you&#39;ve created at the remote site will allow you to view statistics with varying levels of details depending on the service. Due to the use of Javascript to track and record statistics, these are completely platform independent. Omniture&#39;s Site Catalyst - Enterprise level analytics. As far as I know, Site Catalyst is really the only player at this level in the market. I don&#39;t know a whole lot about it, but as I get access to it later on, I&#39;m sure I&#39;ll be writing more about it. Google Analytics - As far as I know, this is the weapon of choice for those that don&#39;t have the resources to implement and use site-catalyst but still want real-time analysis of their web traffic Microsoft&#39;s Gatineau - As of this writing, Gatineau isn&#39;t live yet, so we don&#39;t exactly know how it will work, but you can bet their trying to enter the Google Analytics market. Details are sparse, but there are a few details over at searchenginejournal.com Others - There are many other tools available. Some of the ones I know of are Sitetracker, OneStat (pay service), and StatCounter Log-file Analysis The alternate method of tracking visitors to your site is to use log-file analysis. Every time your HTTP server sends out a page to a web browser, the request is stored in a log file. A log file is simply a text file on your server that contains IP addresses, times, pages requested, and any other number of statistics. Parsing through the log files and comparing certain bits of data to others allows a detailed picture of the traffic on your website. Because these tools actually live on the servers andlook through log files, they are OS dependent. AWStats - As far as I know, AWStats is the major player in this market. It&#39;s an open-source tool that has been around since before I was doing web development. I don&#39;t have extensive experience with AWStats, but my understanding is that the level of detail is not quite on par with Google Analytics. AWStats works by running a PERL (maybe Python) script that parses through your server log files to generate static HTML pages. A cron job runs the script whenever you schedule it, and then the stats are available at http://yourdomain/stats. There is also a CGI interface that will do it in real-time for you. Others - There are many tools that do this type of job. See the Wikipedia for more options in this arena. A post from Jimmy Zimmerman highlights one issue with log-file analysis. Blogs and other types of web-sites that encourage user feedback have forms for posting comments, etc. These comment forms are highly targeted by comment spammers that use this method of feedback for free advertising. Since these spams consist of valid Request/Response pairs, they pollute your log files with false data. If you use log-file analysis to track statistics on your site, be aware of this and take steps to cleanse your logs to produce accurate stats.","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Your awesome title" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Your awesome title</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about-me.html">About Me</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Web Traffic Analysis</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2007-01-24T00:04:56-07:00" itemprop="datePublished">Jan 24, 2007
      </time>â€¢ <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">Ben</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In the realm of analyzing traffic that your web-site is generating, there are two main approaches.  Log-file analysis &amp; real-time analysis (I don't know if that's an industry term, but that's what I'm going to call it).</p>
<p><strong>Real-Time Analysis</strong><br />
As a general rule, the real-time analysis tools use javascript to track visitors to your website.  Pasting a bit of javascript into the header of your HTML pages (or templates) means that every time a page is loaded, your website will send a call to a remote database somewhere with information about the visitor.  Logging into the account you've created at the remote site will allow you to view statistics with varying levels of details depending on the service.  Due to the use of Javascript to track and record statistics, these are completely platform independent.</p>
<ul>
<li>Omniture's <a href="http://www.omniture.com/products/web_analytics/sitecatalyst" title="Site Catalyst">Site Catalyst</a> - Enterprise level analytics.  As far as I know, Site Catalyst is really the only player at this level in the market.  I don't know a whole lot about it, but as I get access to it later on, I'm sure I'll be writing more about it.</li>
<li><a href="https://www.google.com/analytics/home/" title="google analytics">Google Analytics</a> - As far as I know, this is the weapon of choice for those that don't have the resources to implement and use site-catalyst but still want real-time analysis of their web traffic</li>
<li>Microsoft's Gatineau - As of this writing, Gatineau isn't live yet, so we don't exactly know how it will work,  but you can bet their trying to enter the Google Analytics market.  Details are sparse, but there are a few details over at <a href="http://www.searchenginejournal.com/?p=4239" title="searchenginejournal.com">searchenginejournal.com</a></li>
<li>Others - There are many other tools available.  Some of the ones I know of are <a href="http://sitetracker.com/" title="sitetracker">Sitetracker</a>, <a href="http://www.onestat.com/" title="onestat">OneStat</a> (pay service), and <a href="http://www.statcounter.com/" title="statcounter">StatCounter</a></li>
</ul>
<p><strong>Log-file Analysis</strong><br />
The alternate method of tracking visitors to your site is to use log-file analysis.  Every time your HTTP server sends out a page to a web browser, the request is stored in a log file.  A log file is simply a text file on your server that contains IP addresses, times, pages requested, and any other number of statistics.  Parsing through the log files and comparing certain bits of data to others allows a detailed picture of the traffic on your website.  Because these tools actually live on the servers andlook through log files, they are OS dependent.</p>
<ul>
<li><a href="http://awstats.sourceforge.net/" title="awstats">AWStats</a> - As far as I know, AWStats is the major player in this market.  It's an open-source tool that has been around since before I was doing web development.  I don't have extensive experience with AWStats, but my understanding is that the level of detail is not quite on par with Google Analytics.  AWStats works by running a PERL (maybe Python) script that parses through your server log files to generate static HTML pages.  A cron job runs the script whenever you schedule it, and then the stats are available at http://yourdomain/stats.  There is also a CGI interface that will do it in real-time for you.</li>
<li>Others - There are many tools that do this type of job.  See the <a href="http://en.wikipedia.org/wiki/Web_log_analysis_software" title="wikipedia web log file analysis software">Wikipedia</a> for more options in this arena.</li>
</ul>
<p>A post from Jimmy Zimmerman <a href="http://jimmyzimmerman.com/blog/2007/01/analytics-by-log-files.html" title="Jimmy Zimmerman's blog">highlights one issue with log-file analysis</a>.  Blogs and other types of web-sites that encourage user feedback have forms for posting comments, etc.  These comment forms are highly targeted by comment spammers that use this method of feedback for free advertising.  Since these spams consist of valid Request/Response pairs, they pollute your log files with false data.  If you use log-file analysis to track statistics on your site, be aware of this and take steps to cleanse your logs to produce accurate stats.</p>

  </div><a class="u-url" href="/2007/01/24/web-traffic-analysis/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Your awesome title</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Your awesome title</li><li><a class="u-email" href="mailto:your-email@example.com">your-email@example.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
